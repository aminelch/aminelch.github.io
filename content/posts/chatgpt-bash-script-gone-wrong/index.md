---
date: 2025-09-01
title: >
  Le pair scripting avec ChatGPT peut brisÃ© ton OS!
description: |
  Jâ€™ai testÃ© le pair scripting avec ChatGPT ; certains scripts Bash peuvent rÃ©ellement briser votre OS si exÃ©cutÃ©s sans vigilance
tags: ["linux", "sysadmin", "bash", "chatgpt", "ia"]
coverAlt: >
    Photo de [Berke Citak](https://unsplash.com/@berctk) sur [Unsplash](https://unsplash.com/)
coverCaption: >
    Photo de [Berke Citak](https://unsplash.com/@berctk) libre en [License](https://unsplash.com/license)
---

> Un script Bash peut sembler anodin jusquâ€™au moment oÃ¹ il menace de rendre ton systÃ¨me **inutilisable**.

Un soir, un peu par curiositÃ©, jâ€™ai demandÃ© Ã  ChatGPT de mâ€™aider Ã  Ã©crire un script pour modifier le shell par dÃ©faut de tous les utilisateurs sur une Debian. Rien de bien sorcier, pensais-je.

Le rÃ©sultat ? Un script simple, efficace et terriblement **dangereux**.
Pourquoi ? Parce quâ€™il appliquait la modification Ã  tous les comptes y compris le compte `root`` compris, mais aussi les comptes systÃ¨me. Autrement dit, un copier/coller un peu trop confiant, et je me retrouvais avec un root inutilisable, des comptes systÃ¨me cassÃ©s.

Câ€™est Ã  ce moment-lÃ  que jâ€™ai compris que l'IA peut Ãªtre un excellent compagnon de pair programming, mais elle ne connaÃ®t ni ton contexte, ni tes contraintes de sÃ©curitÃ©.

![](prompt-gpt1.png)
![](prompt-gpt2.png)

<!-- La vraie valeur, câ€™est lâ€™humain qui revoit, corrige et ajoute les garde-fous. -->
Dans mon cas, jâ€™ai modifiÃ© le script pour :

- Exclure le compte `root`,
- Ignorer les faux shells (`/usr/sbin/nologin`, `/bin/false`),
- Cibler uniquement les vrais utilisateurs.

Finalement, parce quâ€™entre nous, si quelquâ€™un doit casser `/etc/passwd`, je prÃ©fÃ¨re que ce soit moi ğŸ˜ au moins je saurai Ã  qui mâ€™en prendre. ğŸ˜…